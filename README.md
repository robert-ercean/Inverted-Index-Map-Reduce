# Inverted Index using Map-Reduce

This project is the construction of an **Inverted Index** over a collection of text files using a simplified **Map-Reduce paradigm**. The threads are split into Mappers and Reducers, each running in parallel to process input text files and produce an inverted index that maps each word to the list of file IDs in which it appears. Ultimately, each character in the alphabet will have a separate file consisting of a list of <word, file_ids> pairs, sorted by number of file appearances, then alphabetically.

## Table of Contents

- [Overview](#overview)
- [Code Structure](#code-structure)
- [Data Structures](#data-structures)
- [Key Components](#key-components)
  - [Mapper](#mapper)
  - [Reducer](#reducer)
  - [filesControlBlock](#filescontrolblock)
- [Execution](#execution)

## Overview

This code simulates a parallelized approach to building an inverted index using principles inspired by the MapReduce Hadoop framework. The process flow is explained below:

1. **Mapping Phase:** Multiple mapper threads process subsets of input files. Each mapper extracts words from its assigned files, normalizes them (removing non-alphabetic characters and converting uppercase letters to lowercase), and then builds partial inverted indices mapping words to file IDs.

2. **Shuffling/Partitioning:** The partial results from each mapper are distributed into character-based buckets. Each word is placed into a bucket based on its starting character.

3. **Reducing Phase:** Each reducer is responsible for a subset of the alphabet and consolidates entries from all mappers. Multiple reducer threads merge and sort their assigned subsets of character buckets to produce the final inverted index file of that respective character.

The output consists of per-character files (`a.txt`, `b.txt`, ... `z.txt`) that list each word starting with that character and the IDs of the files in which the word appears.

## Code Structure

The code files and their meaning:

- **main.cpp**: Initializes the input, sets up the environment (the Mapper and Reducer threads count, memory pre-allocation, input files, pthread library primitives intialization and clean-up), starts the Mapper and Reducer threads and joins them.
- **Mapper.h / Mapper.cpp**: Defines the `Mapper` class as an extension of the AbstractThread class. It reads assigned files, processes words, and stores partial results.
- **Reducer.h / Reducer.cpp**: Defines the `Reducer` class as an extension of the AbstractThread class. It merges mapper outputs and writes the final inverted index to disk.
- **structs.h**: Defines common data structures, including `filesControlBlock`, `entry`, `file`, and helper functions such as comparators for the heaps.
- **AbstractThread.h / AbstractThread.cpp**: A generic thread wrapper to manage threads in an elegant C++ OOP style while still using the pthread library functions and primitives. 

## Data Structures

- **`file`**: Stores metadata about an input file (`filename`, `id`, `size`).
- **`entry`**: Represents a pair of type `<key, value>`, where the `key` is the **word** and `value` is the `vector of file ID's` in which the respective word appears.
- **`filesControlBlock (fcb)`**: Central shared structure that holds:
  - Shared `Reduce Barrier` for synchronization (the Reducers start working only after all Mappers finished their tasks)
  - Arrays for partial entries generated by mappers
    - *The partialEntries 3-dimensional matrix is indexed as follows: **[ALPHABET_CHAR][MAPPER_ID][RANDOM_ENTRY_ID]**. So, each mapper writes its parsed entries in an isolated memory location to avoid the need to use mutexes or other synchronization mechanisms as they would add an extra overhead and decrease the speed-up.*
  - Priority queues array for storing the merged heaps for easier access when writing the files to disk.
  - Character frequency arrays(split into multiple subarray, one for each Mapper) for balancing load among reducers

## Key Components

### Mapper

- Each `Mapper` is given a subset of input files based on a greedy load-balancing strategy, as follows:
    - The main thread reads the input filenames and their size using the <filesystem> library, then sorts descending by their size. It assigns each Mapper a workload of 0 and then iterates over the file array and at each step assigns the current Mapper with the least workload the highest size file. *This greedy approach reflects a static load balancing strategy, but I chose it over a dynamic one because it produced better speed-up numbers over a larger number of Mappers, at least in the context of this assignment.*
 - Mapper workflow:
  - Reads all assigned files.
  - Extracts words, normalizes them (keep only alphabetic characters, convert to lowercase).
  - Accumulates occurrences of these words and stores them in `partialEntries`, indexed by the Mapper's own ID.
- After processing, it waits at a barrier that ensures all mappers are done before reducers start.

### Reducer

- Each `Reducer` waits until all mappers finish.
- It:
  - Assigns characters of the alphabet among reducers based on a greedy load-balancing strategy, similar to the Mapper one.
    - The algorithm is very similar to the Mapper one, the key differences being that the workload of each Reducer will be reflected by the total sum of assigned character frequencies and a specific-character frequency is obtained by summing over all frequencties generated by all the Mapper threads.
  - Merges all `partialEntries` for its assigned characters, combining identical key words by merging their fileID's array and sorting it.
  - Writes the results into character-based output files (e.g., `a.txt` for words starting with 'a').

### filesControlBlock

- Shared state among all mappers and reducers:
  - `partialEntries[ch][mapperId][someEntry]`: partial results for words starting with `ch` from a specific mapper.
  - `mergedHeaps[ch]`: a priority queue for merging entries for words starting with `ch`.
  - `chFreq`: frequency arrays used by reducers for load distribution. Indexed by [MAPPER_COUNT][CHAR] for the same reason the partialEntries 3d matrix is indexed by the Mapper ID: to avoid synchronization overhead.
  - Barriers (`reduceBarrier`) to synchronize the Mapping and Reducing phases.
  

## Execution

- Use at least c++17: 
```bash
g++ -std=c++17 -g -Wall -Werror main.cpp -o inverted_index -lpthread
```

- The expected arguments are as follows:
    - <NUM_MAPPERS>: The number of mapper threads.
    - <NUM_REDUCERS>: The number of reducer threads.
    - <FILENAMES_LIST>: A file that contains a list of input filenames (one per line) after a header line(the count of filenames).

```bash
./inverted_index <NUM_MAPPERS> <NUM_REDUCERS> <FILENAMES_LIST>
```